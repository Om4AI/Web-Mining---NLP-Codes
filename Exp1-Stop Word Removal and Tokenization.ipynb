{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1da72510-2ac0-4a95-9bff-82dfab690b16",
   "metadata": {},
   "source": [
    "# *Stop Word Removal & Tokenization - Without NLTK*\n",
    "### *`Code by- @Om`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14a6ef3-9c13-4381-bad3-0bd3f4966723",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5e7476a8-3b7a-4826-8bca-5a44e7989486",
   "metadata": {},
   "source": [
    "## Remove Stopwords from any paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb270c72-cd5c-4055-b48f-842b9f4a5c8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the paragraph: \n",
      " This is just a new sentence, where there are some words and some stopwords included.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----Stopwords removal from given paragraph------\n",
      "\n",
      "RESULT -\n",
      "Paragraph after Stopwords Removal: \n",
      "\n",
      "This is just new sentence where there are some words some stopwords included\n"
     ]
    }
   ],
   "source": [
    "# Removing stopwords from any paragraph\n",
    "# Code by Om\n",
    "\n",
    "# Stopwords list \n",
    "stopw = ['.',',','a','they','the','his','so','and','were','from','that','of','in','only','with','to']\n",
    "para = input(\"Enter the paragraph: \\n\").split()\n",
    "\n",
    "# Remove stopwords\n",
    "for w in para:\n",
    "    if(w in stopw): para.remove(w)\n",
    "\n",
    "# Create temporary result after removing stopwords\n",
    "temp = \"\"\n",
    "for w in para: temp=temp+w+\" \"\n",
    "\n",
    "# Remove comma(,) & (.)\n",
    "res = \"\"\n",
    "for c in temp: \n",
    "    if(c==',' or c=='.'):continue\n",
    "    else: res+=c\n",
    "    \n",
    "print(\"\\n----Stopwords removal from given paragraph------\\n\")\n",
    "\n",
    "print(\"RESULT -\\nParagraph after Stopwords Removal: \\n\\n\"+res.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627cc33b-cc3f-4628-bf57-74402b3e6311",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b7a1a4aa-4c54-4720-a62b-b0f6aacc0df0",
   "metadata": {},
   "source": [
    "## *Tokenization:* Normal Programming Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d2ce730-2d90-4f0b-83f5-d8bcdd1ad60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code by Om\n",
    "# Function to remove stopwords\n",
    "def remove_stopwords(data):\n",
    "    stopw = ['.',',','a','they','the','his','so','and','were','from','that','of','in','only','with','to']\n",
    "    para = data.split()\n",
    "    \n",
    "    # Remove stopwords\n",
    "    for w in para:\n",
    "        if(w in stopw): para.remove(w)\n",
    "    # Create temporary result after removing stopwords\n",
    "    temp = \"\"\n",
    "    for w in para: temp=temp+w+\" \"\n",
    "    # Remove comma(,) & (.)\n",
    "    res = \"\"\n",
    "    for c in temp: \n",
    "        if(c==',' or c=='.' or c=='!'):continue\n",
    "        else: res+=c\n",
    "    return res.strip().split()\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize(words):\n",
    "    tokens = {}\n",
    "    p=0\n",
    "    for w in words:\n",
    "        if w not in tokens.values():\n",
    "            tokens[p] = w\n",
    "            p+=1\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b0dca0a-3678-4ebb-831b-5e5af1894900",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----Tokenization: Without NLTK (Single Sentence)----\n",
      "\n",
      "Sentence: This is just a new sentence, where there are some words and some stopwords included.\n",
      "\n",
      "Number of words-after stopword removal:  13\n",
      "Number of tokens:  12 \n",
      "\n",
      "Token  0 : This\n",
      "Token  1 : is\n",
      "Token  2 : just\n",
      "Token  3 : new\n",
      "Token  4 : sentence\n",
      "Token  5 : where\n",
      "Token  6 : there\n",
      "Token  7 : are\n",
      "Token  8 : some\n",
      "Token  9 : words\n",
      "Token  10 : stopwords\n",
      "Token  11 : included\n"
     ]
    }
   ],
   "source": [
    "# Tokenization: Handling Single Sentence\n",
    "print(\"\\n----Tokenization: Without NLTK (Single Sentence)----\\n\")\n",
    "print(\"Sentence: This is just a new sentence, where there are some words and some stopwords included.\\n\")\n",
    "words = remove_stopwords(\"This is just a new sentence, where there are some words and some stopwords included.\")\n",
    "print(\"Number of words-after stopword removal: \",len(words))\n",
    "res = tokenize(words)\n",
    "\n",
    "print(\"Number of tokens: \", len(res.keys()),\"\\n\")\n",
    "for k in res.keys():\n",
    "    print(\"Token \",k,\":\",res[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a652955-67a4-46a7-9d6e-d1cecd394684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----Tokenization: Without NLTK (Multiple Sentences)----\n",
      "\n",
      "Data:  Google LLC is an American multinational technology company that specializes in Internet-related services and products, which include online advertising technologies, a search engine, cloud computing, software, and hardware. It is considered one of the Big Five companies in the American information technology industry, along with Amazon, Apple, Meta (Facebook) and Microsoft.\n",
      "\n",
      "Number of tokens:  38 \n",
      "\n",
      "Token  0 : Google\n",
      "Token  1 : LLC\n",
      "Token  2 : is\n",
      "Token  3 : an\n",
      "Token  4 : American\n",
      "Token  5 : multinational\n",
      "Token  6 : technology\n",
      "Token  7 : company\n",
      "Token  8 : specializes\n",
      "Token  9 : Internet-related\n",
      "Token  10 : services\n",
      "Token  11 : products\n",
      "Token  12 : which\n",
      "Token  13 : include\n",
      "Token  14 : online\n",
      "Token  15 : advertising\n",
      "Token  16 : technologies\n",
      "Token  17 : search\n",
      "Token  18 : engine\n",
      "Token  19 : cloud\n",
      "Token  20 : computing\n",
      "Token  21 : software\n",
      "Token  22 : hardware\n",
      "Token  23 : It\n",
      "Token  24 : considered\n",
      "Token  25 : one\n",
      "Token  26 : the\n",
      "Token  27 : Big\n",
      "Token  28 : Five\n",
      "Token  29 : companies\n",
      "Token  30 : information\n",
      "Token  31 : industry\n",
      "Token  32 : along\n",
      "Token  33 : Amazon\n",
      "Token  34 : Apple\n",
      "Token  35 : Meta\n",
      "Token  36 : (Facebook)\n",
      "Token  37 : Microsoft\n"
     ]
    }
   ],
   "source": [
    "# Tokenization: Handling Multiple sentences\n",
    "print(\"\\n----Tokenization: Without NLTK (Multiple Sentences)----\\n\")\n",
    "data = open(\"Data_multi.txt\", \"r\")\n",
    "sen = \"\"\n",
    "for l in data:\n",
    "    sen=sen+\" \"+l.strip()\n",
    "print(\"Data: \"+sen)\n",
    "multi_res = tokenize(remove_stopwords(sen))\n",
    "\n",
    "print(\"\\nNumber of tokens: \", len(multi_res.keys()),\"\\n\")\n",
    "for k in multi_res.keys():\n",
    "    print(\"Token \",k,\":\",multi_res[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10aa8c75-322a-4be7-a75a-0af55a91f204",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "16457338-9f0f-4756-a038-9198b3232b08",
   "metadata": {},
   "source": [
    "## *Tokenization:* Using built-in libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e340485-e503-4ccc-a37b-3bbf4a7f32df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## import the libraries\n",
    "# import spacy\n",
    "# ## load the language model\n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# ## tokenization\n",
    "# doc = nlp(\"This is just a new sentence, where there are some words and some stopwords included.\")\n",
    "# for token in doc:\n",
    "#     print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f22674-f368-4d4b-951f-add28fc25382",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd240476-dde4-405d-a3d5-c58de3f9445e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
