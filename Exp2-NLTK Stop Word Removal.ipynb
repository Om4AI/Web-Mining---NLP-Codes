{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57a4faf5-e971-4727-8e6b-01c9d4b1ccac",
   "metadata": {},
   "source": [
    "# *Tokenization, Lemmatization & Stemming - Using NLTK*\n",
    "### *`Code by- @Om`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4db197-aacc-441c-9a7e-c9c5d4830dde",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c2894540-d88c-4d1e-9732-811d5f224e81",
   "metadata": {},
   "source": [
    "## *Tokenization, Lemmatization & Stemming - Single sentence*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2960195-3c80-4035-94ce-a4031e196ee7",
   "metadata": {},
   "source": [
    "## *Tokenization - Using NLTK*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55e7ea30-78de-473a-ad67-be480a1e1d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions/Dependencies\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Stopwords removal function\n",
    "def remove_stopw_nltk(sent):\n",
    "    stopws = stopwords.words('english')\n",
    "    words = sent.split()\n",
    "    print(\"Words before stopwords removal: \",len(words))\n",
    "    sent = \"\"\n",
    "    for w in words:\n",
    "        if w in stopws: words.remove(w)\n",
    "    for w in words:\n",
    "        sent=sent+w+\" \"\n",
    "    print(\"Words after stopwords removal: \",len(words),\"\\n\")\n",
    "    return sent\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize(sent):\n",
    "    sent = remove_stopw_nltk(sent)\n",
    "    print(\"\\nModified Sentence: \",sent,\"\\n\")\n",
    "    tokens = word_tokenize(sent)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2fd3899-5fa0-41cd-b010-a4c09601706f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----Tokenization of given sentence - With NLTK------\n",
      "\n",
      "Original Sentence:  This is a new test sentence for checking tokenization using NLTK package \n",
      "\n",
      "Words before stopwords removal:  12\n",
      "Words after stopwords removal:  10 \n",
      "\n",
      "\n",
      "Modified Sentence:  This a new test sentence checking tokenization using NLTK package  \n",
      "\n",
      "Number of tokens found -  10\n",
      "Tokens:  ['This', 'a', 'new', 'test', 'sentence', 'checking', 'tokenization', 'using', 'NLTK', 'package']\n"
     ]
    }
   ],
   "source": [
    "sent = \"This is a new test sentence for checking tokenization using NLTK package\"\n",
    "print(\"\\n----Tokenization of given sentence - With NLTK------\\n\")\n",
    "print(\"Original Sentence: \",sent,\"\\n\")\n",
    "\n",
    "tokens = tokenize(sent)\n",
    "\n",
    "# Results\n",
    "print(\"Number of tokens found - \", len(tokens))\n",
    "print(\"Tokens: \",tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa4ceff-3bd9-4f7f-8a83-1a859e103187",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "25824f00-617a-4390-a8c2-1ce1e2cfb96d",
   "metadata": {},
   "source": [
    "## *Stemming using NLTK*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b23a00f3-212e-4094-8a26-63c003d2066e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming function\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "def stem_sentence(sent):\n",
    "    ps = PorterStemmer()\n",
    "    sent = remove_stopw_nltk(sent)\n",
    "    words = word_tokenize(sent)\n",
    "    stemmed = {}\n",
    "    for w in words:\n",
    "        stemmed[w] = ps.stem(w)\n",
    "    return stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3d5f1c0-2b45-4f8c-b08b-06722c408437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----Stemming of given sentence - With NLTK------\n",
      "\n",
      "Original Sentence:  This is a new test sentence for checking tokenization using NLTK package \n",
      "\n",
      "Words before stopwords removal:  12\n",
      "Words after stopwords removal:  10 \n",
      "\n",
      "\n",
      "Stemming Results: \n",
      "------------------\n",
      "This  :  thi\n",
      "a  :  a\n",
      "new  :  new\n",
      "test  :  test\n",
      "sentence  :  sentenc\n",
      "checking  :  check\n",
      "tokenization  :  token\n",
      "using  :  use\n",
      "NLTK  :  nltk\n",
      "package  :  packag\n"
     ]
    }
   ],
   "source": [
    "sent = \"This is a new test sentence for checking tokenization using NLTK package\"\n",
    "print(\"\\n----Stemming of given sentence - With NLTK------\\n\")\n",
    "print(\"Original Sentence: \",sent,\"\\n\")\n",
    "\n",
    "stem_dict = stem_sentence(sent)\n",
    "print(\"\\nStemming Results: \")\n",
    "print(\"------------------\")\n",
    "for k in stem_dict:\n",
    "    print(k,\" : \", stem_dict[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812b3a72-54d9-40ca-a5df-bfb2632710d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8f351705-755a-4574-a331-b5a67ba21b32",
   "metadata": {},
   "source": [
    "## *Lemmatization using NLTK*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d9a232a-7d8d-49cb-9663-156fe1dac62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization function\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def lemmatize(sent):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    sent = remove_stopw_nltk(sent)\n",
    "    words = word_tokenize(sent)\n",
    "    lem_dict = {}\n",
    "    for w in words:\n",
    "        lem_dict[w] = lemmatizer.lemmatize(w, pos=\"a\")\n",
    "    return lem_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0be88e5e-ec93-4d4f-bb63-d0db98cbc8b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----Lemmatization of given sentence - With NLTK------\n",
      "\n",
      "Original Sentence:  This is a better test sentence for checking tokenization using NLTK package \n",
      "\n",
      "Words before stopwords removal:  12\n",
      "Words after stopwords removal:  10 \n",
      "\n",
      "Lemmatization Results: \n",
      "------------------\n",
      "This  :  This\n",
      "a  :  a\n",
      "better  :  good\n",
      "test  :  test\n",
      "sentence  :  sentence\n",
      "checking  :  checking\n",
      "tokenization  :  tokenization\n",
      "using  :  using\n",
      "NLTK  :  NLTK\n",
      "package  :  package\n"
     ]
    }
   ],
   "source": [
    "# Calling functions for lemmatization\n",
    "sent = \"This is a better test sentence for checking tokenization using NLTK package\"\n",
    "print(\"\\n----Lemmatization of given sentence - With NLTK------\\n\")\n",
    "print(\"Original Sentence: \",sent,\"\\n\")\n",
    "\n",
    "lem_dict = lemmatize(sent)\n",
    "\n",
    "print(\"Lemmatization Results: \")\n",
    "print(\"------------------\")\n",
    "for k in lem_dict:\n",
    "    print(k,\" : \", lem_dict[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703aa274-38c8-4f31-ace9-275b6bc29803",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1b0200c7-bb3f-4faf-925a-3c072efcd1c6",
   "metadata": {},
   "source": [
    "## *Tokenization, Lemmatization & Stemming - Paragraph (Multiple Sentences)(Using .txt file)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "346aea0f-bc62-45ce-8074-7cb8385aa70a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----Tokenization of paragraph - With NLTK------\n",
      "\n",
      "Words before stopwords removal:  51\n",
      "Words after stopwords removal:  39 \n",
      "\n",
      "\n",
      "Modified Sentence:  Google LLC an American multinational technology company specializes Internet-related services products, include online advertising technologies, search engine, cloud computing, software, hardware. It considered one the Big Five companies the American information technology industry, along Amazon, Apple, Meta (Facebook) Microsoft.  \n",
      "\n",
      "Tokens found are: \n",
      "--------------------\n",
      " ['online', ')', 'the', 'services', ',', 'Apple', 'Microsoft', 'technology', 'companies', 'advertising', 'cloud', '.', 'Internet-related', 'Big', 'It', 'Amazon', 'Facebook', 'an', 'company', 'search', 'industry', 'along', 'technologies', 'considered', 'multinational', 'specializes', 'LLC', 'hardware', 'engine', 'American', 'Google', 'one', 'software', 'products', 'include', 'Meta', '(', 'information', 'Five', 'computing']\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n----Tokenization of paragraph - With NLTK------\\n\")\n",
    "data = open(\"Data_multi.txt\")\n",
    "s = \"\"\n",
    "\n",
    "# Get data from text file\n",
    "for l in data: s+=l;\n",
    "\n",
    "# Tokenization\n",
    "res = list(set(tokenize(s)))\n",
    "print(\"Tokens found are: \")\n",
    "print(\"--------------------\\n\", res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc8dbea-721f-4ea6-b634-f6512ade5d78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86d10a6a-9c4b-4812-9d06-232d453785a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----Stemming of paragraph - With NLTK------\n",
      "\n",
      "Words before stopwords removal:  51\n",
      "Words after stopwords removal:  39 \n",
      "\n",
      "\n",
      "Stemming results: \n",
      "--------------------\n",
      "\n",
      "Google  :  googl\n",
      "LLC  :  llc\n",
      "an  :  an\n",
      "American  :  american\n",
      "multinational  :  multin\n",
      "technology  :  technolog\n",
      "company  :  compani\n",
      "specializes  :  special\n",
      "Internet-related  :  internet-rel\n",
      "services  :  servic\n",
      "products  :  product\n",
      ",  :  ,\n",
      "include  :  includ\n",
      "online  :  onlin\n",
      "advertising  :  advertis\n",
      "technologies  :  technolog\n",
      "search  :  search\n",
      "engine  :  engin\n",
      "cloud  :  cloud\n",
      "computing  :  comput\n",
      "software  :  softwar\n",
      "hardware  :  hardwar\n",
      ".  :  .\n",
      "It  :  it\n",
      "considered  :  consid\n",
      "one  :  one\n",
      "the  :  the\n",
      "Big  :  big\n",
      "Five  :  five\n",
      "companies  :  compani\n",
      "information  :  inform\n",
      "industry  :  industri\n",
      "along  :  along\n",
      "Amazon  :  amazon\n",
      "Apple  :  appl\n",
      "Meta  :  meta\n",
      "(  :  (\n",
      "Facebook  :  facebook\n",
      ")  :  )\n",
      "Microsoft  :  microsoft\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n----Stemming of paragraph - With NLTK------\\n\")\n",
    "data = open(\"Data_multi.txt\")\n",
    "s = \"\"\n",
    "\n",
    "# Get data from text file\n",
    "for l in data: s+=l;\n",
    "\n",
    "# Stemming\n",
    "res = stem_sentence(s)\n",
    "print(\"\\nStemming results: \")\n",
    "print(\"--------------------\\n\")\n",
    "for k in res:\n",
    "    print(k,\" : \",res[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea634c44-9866-48fc-b013-beccd6da5ff5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f5f95aff-4c94-474d-9de5-3cde3deb1794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----Lemmatization of paragraph - With NLTK------\n",
      "\n",
      "Words before stopwords removal:  51\n",
      "Words after stopwords removal:  39 \n",
      "\n",
      "\n",
      "Lemmatization results: \n",
      "--------------------\n",
      "\n",
      "Google  :  Google\n",
      "LLC  :  LLC\n",
      "an  :  an\n",
      "American  :  American\n",
      "multinational  :  multinational\n",
      "technology  :  technology\n",
      "company  :  company\n",
      "specializes  :  specializes\n",
      "Internet-related  :  Internet-related\n",
      "services  :  services\n",
      "products  :  products\n",
      ",  :  ,\n",
      "include  :  include\n",
      "online  :  online\n",
      "advertising  :  advertising\n",
      "technologies  :  technologies\n",
      "search  :  search\n",
      "engine  :  engine\n",
      "cloud  :  cloud\n",
      "computing  :  computing\n",
      "software  :  software\n",
      "hardware  :  hardware\n",
      ".  :  .\n",
      "It  :  It\n",
      "considered  :  considered\n",
      "one  :  one\n",
      "the  :  the\n",
      "Big  :  Big\n",
      "Five  :  Five\n",
      "companies  :  companies\n",
      "information  :  information\n",
      "industry  :  industry\n",
      "along  :  along\n",
      "Amazon  :  Amazon\n",
      "Apple  :  Apple\n",
      "Meta  :  Meta\n",
      "(  :  (\n",
      "Facebook  :  Facebook\n",
      ")  :  )\n",
      "Microsoft  :  Microsoft\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n----Lemmatization of paragraph - With NLTK------\\n\")\n",
    "data = open(\"Data_multi.txt\")\n",
    "s = \"\"\n",
    "\n",
    "# Get data from text file\n",
    "for l in data: s+=l;\n",
    "\n",
    "# Stemming\n",
    "res = lemmatize(s)\n",
    "print(\"\\nLemmatization results: \")\n",
    "print(\"--------------------\\n\")\n",
    "for k in res:\n",
    "    print(k,\" : \",res[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311f3313-01bc-4684-8025-e33bdc3254a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7b5e06e3-0bc1-4409-9899-a0f0b5c54ec0",
   "metadata": {},
   "source": [
    "## *Tokenization, Lemmatization & Stemming - Multiple Documents (Multiple Sentences)(Using .txt files)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a113573c-6c7a-44c5-ba58-a2be7a7f3e97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----Tokenization of multiple documents - With NLTK------\n",
      "\n",
      "\n",
      "\n",
      "--------- Document  1 -------------\n",
      "\n",
      "Words before stopwords removal:  51\n",
      "Words after stopwords removal:  39 \n",
      "\n",
      "\n",
      "Modified Sentence:  Google LLC an American multinational technology company specializes Internet-related services products, include online advertising technologies, search engine, cloud computing, software, hardware. It considered one the Big Five companies the American information technology industry, along Amazon, Apple, Meta (Facebook) Microsoft.  \n",
      "\n",
      "Tokens found:  40\n",
      "--------------------\n",
      " ['online', ')', 'the', 'services', ',', 'Apple', 'Microsoft', 'technology', 'companies', 'advertising', 'cloud', '.', 'Internet-related', 'Big', 'It', 'Amazon', 'Facebook', 'an', 'company', 'search', 'industry', 'along', 'technologies', 'considered', 'multinational', 'specializes', 'LLC', 'hardware', 'engine', 'American', 'Google', 'one', 'software', 'products', 'include', 'Meta', '(', 'information', 'Five', 'computing'] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--------- Document  2 -------------\n",
      "\n",
      "Words before stopwords removal:  38\n",
      "Words after stopwords removal:  29 \n",
      "\n",
      "\n",
      "Modified Sentence:  Meta Platforms, Inc. also known Meta[13] formerly known Facebook, Inc., an American multinational technology conglomerate based Menlo Park, California. The company the parent organization Facebook, Instagram, WhatsApp, among subsidiaries.  \n",
      "\n",
      "Tokens found:  30\n",
      "--------------------\n",
      " ['the', 'WhatsApp', ']', 'parent', ',', 'Inc.', 'technology', 'The', '[', 'California', '.', 'Menlo', 'also', 'conglomerate', 'known', 'Facebook', 'an', 'company', 'subsidiaries', 'formerly', 'based', 'organization', '13', 'among', 'multinational', 'Platforms', 'American', 'Meta', 'Park', 'Instagram'] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--------- Document  3 -------------\n",
      "\n",
      "Words before stopwords removal:  38\n",
      "Words after stopwords removal:  26 \n",
      "\n",
      "\n",
      "Modified Sentence:  Netflix, Inc. an American subscription streaming service production company. Launched August 29, 1997, offers library films television series distribution deals well its productions, known Netflix Originals.  \n",
      "\n",
      "Tokens found:  27\n",
      "--------------------\n",
      " ['Launched', 'productions', 'deals', ',', 'Inc.', 'distribution', 'library', 'its', 'August', 'television', '29', 'service', '.', 'Netflix', 'offers', 'films', 'known', 'subscription', 'production', 'an', 'company', 'series', 'well', 'streaming', '1997', 'American', 'Originals'] \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n----Tokenization of multiple documents - With NLTK------\\n\")\n",
    "doc_names = [\"Data_multi.txt\", \"Data_2.txt\", \"Data_3.txt\"]\n",
    "\n",
    "c=1\n",
    "# Loop to get data and tokenize\n",
    "for doc in doc_names:\n",
    "    print(\"\\n\\n--------- Document \",c,\"-------------\\n\")\n",
    "    data = open(doc)\n",
    "    s = \"\"\n",
    "    \n",
    "    # Get data from text file\n",
    "    for l in data: s+=l;\n",
    "    \n",
    "    # Tokenization\n",
    "    res = list(set(tokenize(s)))\n",
    "    print(\"Tokens found: \", len(res))\n",
    "    print(\"--------------------\\n\", res,\"\\n\\n\")\n",
    "    c+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43dbedc5-a289-49e7-92e9-d029ee8d55f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4ada2d6e-cd0e-4d53-bdd7-c5ab7b8b8c75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----Stemming of multiple documents - With NLTK------\n",
      "\n",
      "\n",
      "\n",
      "--------- Document  1 -------------\n",
      "\n",
      "Words before stopwords removal:  51\n",
      "Words after stopwords removal:  39 \n",
      "\n",
      "Stemming results: \n",
      "--------------------\n",
      " {'Google': 'googl', 'LLC': 'llc', 'an': 'an', 'American': 'american', 'multinational': 'multin', 'technology': 'technolog', 'company': 'compani', 'specializes': 'special', 'Internet-related': 'internet-rel', 'services': 'servic', 'products': 'product', ',': ',', 'include': 'includ', 'online': 'onlin', 'advertising': 'advertis', 'technologies': 'technolog', 'search': 'search', 'engine': 'engin', 'cloud': 'cloud', 'computing': 'comput', 'software': 'softwar', 'hardware': 'hardwar', '.': '.', 'It': 'it', 'considered': 'consid', 'one': 'one', 'the': 'the', 'Big': 'big', 'Five': 'five', 'companies': 'compani', 'information': 'inform', 'industry': 'industri', 'along': 'along', 'Amazon': 'amazon', 'Apple': 'appl', 'Meta': 'meta', '(': '(', 'Facebook': 'facebook', ')': ')', 'Microsoft': 'microsoft'} \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--------- Document  2 -------------\n",
      "\n",
      "Words before stopwords removal:  38\n",
      "Words after stopwords removal:  29 \n",
      "\n",
      "Stemming results: \n",
      "--------------------\n",
      " {'Meta': 'meta', 'Platforms': 'platform', ',': ',', 'Inc.': 'inc.', 'also': 'also', 'known': 'known', '[': '[', '13': '13', ']': ']', 'formerly': 'formerli', 'Facebook': 'facebook', 'an': 'an', 'American': 'american', 'multinational': 'multin', 'technology': 'technolog', 'conglomerate': 'conglomer', 'based': 'base', 'Menlo': 'menlo', 'Park': 'park', 'California': 'california', '.': '.', 'The': 'the', 'company': 'compani', 'the': 'the', 'parent': 'parent', 'organization': 'organ', 'Instagram': 'instagram', 'WhatsApp': 'whatsapp', 'among': 'among', 'subsidiaries': 'subsidiari'} \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--------- Document  3 -------------\n",
      "\n",
      "Words before stopwords removal:  38\n",
      "Words after stopwords removal:  26 \n",
      "\n",
      "Stemming results: \n",
      "--------------------\n",
      " {'Netflix': 'netflix', ',': ',', 'Inc.': 'inc.', 'an': 'an', 'American': 'american', 'subscription': 'subscript', 'streaming': 'stream', 'service': 'servic', 'production': 'product', 'company': 'compani', '.': '.', 'Launched': 'launch', 'August': 'august', '29': '29', '1997': '1997', 'offers': 'offer', 'library': 'librari', 'films': 'film', 'television': 'televis', 'series': 'seri', 'distribution': 'distribut', 'deals': 'deal', 'well': 'well', 'its': 'it', 'productions': 'product', 'known': 'known', 'Originals': 'origin'} \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n----Stemming of multiple documents - With NLTK------\\n\")\n",
    "doc_names = [\"Data_multi.txt\", \"Data_2.txt\", \"Data_3.txt\"]\n",
    "\n",
    "c=1\n",
    "# Loop to get data and tokenize\n",
    "for doc in doc_names:\n",
    "    print(\"\\n\\n--------- Document \",c,\"-------------\\n\")\n",
    "    data = open(doc)\n",
    "    s = \"\"\n",
    "    \n",
    "    # Get data from text file\n",
    "    for l in data: s+=l;\n",
    "    \n",
    "    # Tokenization\n",
    "    res = stem_sentence(s)\n",
    "    print(\"Stemming results: \")\n",
    "    print(\"--------------------\\n\", res,\"\\n\\n\")\n",
    "    c+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39e7651-83ed-4622-961a-4bf8b80de798",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a0907cf-1324-4407-8fcf-c4b971e896ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----Lemmatization of multiple documents - With NLTK------\n",
      "\n",
      "\n",
      "\n",
      "--------- Document  1 -------------\n",
      "\n",
      "Words before stopwords removal:  51\n",
      "Words after stopwords removal:  39 \n",
      "\n",
      "Lemmatization results: \n",
      "--------------------\n",
      " {'Google': 'Google', 'LLC': 'LLC', 'an': 'an', 'American': 'American', 'multinational': 'multinational', 'technology': 'technology', 'company': 'company', 'specializes': 'specializes', 'Internet-related': 'Internet-related', 'services': 'services', 'products': 'products', ',': ',', 'include': 'include', 'online': 'online', 'advertising': 'advertising', 'technologies': 'technologies', 'search': 'search', 'engine': 'engine', 'cloud': 'cloud', 'computing': 'computing', 'software': 'software', 'hardware': 'hardware', '.': '.', 'It': 'It', 'considered': 'considered', 'one': 'one', 'the': 'the', 'Big': 'Big', 'Five': 'Five', 'companies': 'companies', 'information': 'information', 'industry': 'industry', 'along': 'along', 'Amazon': 'Amazon', 'Apple': 'Apple', 'Meta': 'Meta', '(': '(', 'Facebook': 'Facebook', ')': ')', 'Microsoft': 'Microsoft'} \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--------- Document  2 -------------\n",
      "\n",
      "Words before stopwords removal:  38\n",
      "Words after stopwords removal:  29 \n",
      "\n",
      "Lemmatization results: \n",
      "--------------------\n",
      " {'Meta': 'Meta', 'Platforms': 'Platforms', ',': ',', 'Inc.': 'Inc.', 'also': 'also', 'known': 'known', '[': '[', '13': '13', ']': ']', 'formerly': 'formerly', 'Facebook': 'Facebook', 'an': 'an', 'American': 'American', 'multinational': 'multinational', 'technology': 'technology', 'conglomerate': 'conglomerate', 'based': 'based', 'Menlo': 'Menlo', 'Park': 'Park', 'California': 'California', '.': '.', 'The': 'The', 'company': 'company', 'the': 'the', 'parent': 'parent', 'organization': 'organization', 'Instagram': 'Instagram', 'WhatsApp': 'WhatsApp', 'among': 'among', 'subsidiaries': 'subsidiaries'} \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--------- Document  3 -------------\n",
      "\n",
      "Words before stopwords removal:  38\n",
      "Words after stopwords removal:  26 \n",
      "\n",
      "Lemmatization results: \n",
      "--------------------\n",
      " {'Netflix': 'Netflix', ',': ',', 'Inc.': 'Inc.', 'an': 'an', 'American': 'American', 'subscription': 'subscription', 'streaming': 'streaming', 'service': 'service', 'production': 'production', 'company': 'company', '.': '.', 'Launched': 'Launched', 'August': 'August', '29': '29', '1997': '1997', 'offers': 'offers', 'library': 'library', 'films': 'films', 'television': 'television', 'series': 'series', 'distribution': 'distribution', 'deals': 'deals', 'well': 'well', 'its': 'its', 'productions': 'productions', 'known': 'known', 'Originals': 'Originals'} \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n----Lemmatization of multiple documents - With NLTK------\\n\")\n",
    "doc_names = [\"Data_multi.txt\", \"Data_2.txt\", \"Data_3.txt\"]\n",
    "\n",
    "c=1\n",
    "# Loop to get data and tokenize\n",
    "for doc in doc_names:\n",
    "    print(\"\\n\\n--------- Document \",c,\"-------------\\n\")\n",
    "    data = open(doc)\n",
    "    s = \"\"\n",
    "    \n",
    "    # Get data from text file\n",
    "    for l in data: s+=l;\n",
    "    \n",
    "    # Tokenization\n",
    "    res = lemmatize(s)\n",
    "    print(\"Lemmatization results: \")\n",
    "    print(\"--------------------\\n\", res,\"\\n\\n\")\n",
    "    c+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53120525-f6ba-4c97-8eae-3d8700f28b42",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
